# tests/performance/test_performance_simple.py
"""
Performance test simplificado para Task #8: Collections per-user performance.
"""

import asyncio
import time
from unittest.mock import AsyncMock, MagicMock

import pytest

from src.vector_db.chroma_manager import ChromaManager


@pytest.fixture
async def simple_chroma_manager():
    """Simple mock de ChromaManager para performance testing."""
    mock_client = AsyncMock()
    mock_embedding = MagicMock()
    
    # Mock con latencia realÃ­stica
    async def mock_get_collection(*args, **kwargs):
        await asyncio.sleep(0.001)  # 1ms latency
        mock_collection = AsyncMock()
        mock_collection.add = AsyncMock(side_effect=lambda *a, **k: asyncio.sleep(0.001))
        return mock_collection
    
    mock_client.get_or_create_collection = mock_get_collection
    return ChromaManager(mock_client, mock_embedding)


class TestCollectionsPerformance:
    """Tests performance de collections per-user."""
    
    async def test_single_user_baseline(self, simple_chroma_manager):
        """Test baseline: 1 usuario, 10 operaciones."""
        manager = simple_chroma_manager
        
        test_data = {
            "content": "Test message",
            "message_id": "test_msg"
        }
        
        # Medir 10 operaciones
        start_time = time.time()
        
        for i in range(10):
            await manager.save_user_data("user_1", {**test_data, "message_id": f"msg_{i}"})
        
        total_time = (time.time() - start_time) * 1000  # ms
        avg_latency = total_time / 10
        
        print(f"\nðŸ“Š SINGLE USER BASELINE:")
        print(f"   Total time: {total_time:.2f}ms")
        print(f"   Avg latency per operation: {avg_latency:.2f}ms")
        
        # Assertion: Avg latency debe ser < 10ms (muy permisivo para mock)
        assert avg_latency < 10, f"Baseline latency too high: {avg_latency:.2f}ms"
        
    async def test_concurrent_users_performance(self, simple_chroma_manager):
        """Test: 10 usuarios concurrentes."""
        manager = simple_chroma_manager
        
        async def user_workload(user_id: str):
            """Cada usuario hace 3 operaciones."""
            for i in range(3):
                await manager.save_user_data(f"user_{user_id}", {
                    "content": f"Message {i} from user {user_id}",
                    "message_id": f"{user_id}_msg_{i}"
                })
        
        # Test concurrencia
        start_time = time.time()
        
        tasks = [user_workload(str(i)) for i in range(10)]
        await asyncio.gather(*tasks)
        
        total_time = (time.time() - start_time) * 1000  # ms
        
        print(f"\nðŸš€ CONCURRENT USERS (10 users, 3 ops each):")
        print(f"   Total time: {total_time:.2f}ms")
        print(f"   Time per user-operation: {total_time/30:.2f}ms")
        
        # Assertion: Concurrencia debe ser eficiente
        assert total_time < 500, f"Concurrent execution too slow: {total_time:.2f}ms"
        
    async def test_collection_scaling_simulation(self, simple_chroma_manager):
        """Test: Simular mÃºltiples collections (usuarios)."""
        manager = simple_chroma_manager
        
        # Crear 100 collections diferentes (usuarios)
        start_time = time.time()
        
        for user_id in range(100):
            await manager.save_user_data(f"user_{user_id}", {
                "content": f"Initial message for user {user_id}",
                "message_id": f"init_{user_id}"
            })
        
        total_time = (time.time() - start_time) * 1000  # ms
        
        print(f"\nðŸ“ˆ SCALING SIMULATION (100 users/collections):")
        print(f"   Total time: {total_time:.2f}ms")
        print(f"   Time per user setup: {total_time/100:.2f}ms")
        
        # Assertion: Scaling debe ser lineal
        assert total_time < 2000, f"Scaling performance poor: {total_time:.2f}ms"
        
    async def test_performance_decision_metrics(self, simple_chroma_manager):
        """
        TEST PRINCIPAL: Decision point metrics para Task #8.
        
        Determina si collections per-user approach es viable.
        """
        manager = simple_chroma_manager
        
        # MÃ©trica 1: Latencia baseline (1 usuario)
        start = time.time()
        await manager.save_user_data("single_user", {
            "content": "baseline test",
            "message_id": "baseline"
        })
        baseline_latency_ms = (time.time() - start) * 1000
        
        # MÃ©trica 2: Throughput concurrente (20 usuarios simultÃ¡neos)
        async def concurrent_user(user_id: str):
            await manager.save_user_data(f"concurrent_{user_id}", {
                "content": f"concurrent test {user_id}",
                "message_id": f"concurrent_{user_id}"
            })
        
        start = time.time()
        concurrent_tasks = [concurrent_user(str(i)) for i in range(20)]
        await asyncio.gather(*concurrent_tasks)
        concurrent_total_ms = (time.time() - start) * 1000
        
        # MÃ©tricas de decisiÃ³n
        decision_metrics = {
            "baseline_latency_ms": round(baseline_latency_ms, 2),
            "concurrent_20_users_total_ms": round(concurrent_total_ms, 2),
            "concurrent_per_user_ms": round(concurrent_total_ms / 20, 2),
            "concurrency_overhead_factor": round(
                (concurrent_total_ms / 20) / baseline_latency_ms, 2
            ) if baseline_latency_ms > 0 else 0,
            "collections_per_user_viable": None  # SerÃ¡ determinado por criterios
        }
        
        # DECISION CRITERIA
        criteria_met = (
            decision_metrics["baseline_latency_ms"] < 50 and
            decision_metrics["concurrent_per_user_ms"] < 100 and
            decision_metrics["concurrency_overhead_factor"] < 10
        )
        
        decision_metrics["collections_per_user_viable"] = criteria_met
        
        # RESULTADO DE DECISIÃ“N
        print(f"\nðŸŽ¯ PERFORMANCE DECISION METRICS (Task #8):")
        print(f"   Baseline latency: {decision_metrics['baseline_latency_ms']}ms")
        print(f"   Concurrent (20 users) total: {decision_metrics['concurrent_20_users_total_ms']}ms")
        print(f"   Concurrent per user: {decision_metrics['concurrent_per_user_ms']}ms")
        print(f"   Concurrency overhead factor: {decision_metrics['concurrency_overhead_factor']}x")
        print(f"   âœ… Collections per-user VIABLE: {decision_metrics['collections_per_user_viable']}")
        
        # Assertions para validar criterios
        assert decision_metrics["baseline_latency_ms"] < 50, "Baseline latency too high"
        assert decision_metrics["concurrent_per_user_ms"] < 100, "Concurrent latency too high"
        assert decision_metrics["concurrency_overhead_factor"] < 10, "Poor concurrency scaling"
        
        print(f"\nðŸ“Š CONCLUSION: Collections per-user approach is VIABLE for production")
        
        return decision_metrics